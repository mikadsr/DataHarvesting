---
title: "Data Harvesting final"
format: html
editor: visual
---

pakcages

```{r echo=FALSE}
#install.packages("scrapex")
#install.packages("magrittr")
library(scrapex)
library(xml2)
library(magrittr)
library(lubridate)
library(dplyr)
library(tidyr)
library(httr)
library(rvest)
library(janitor)
library(stringr)
library(stringr)
library(tidyverse)

```

```{r}
sider_link<- "http://sideeffects.embl.de/drugs/"
```

docker pull selenium/standalone-firefox:2.53.0 sido docker -d -p 4445:4444 selenium/standalone-firefox:2.53.0

## Setting up Selenium browser

To do before

-   Download Java [here](https://www.oracle.com/java/technologies/downloads/)

-   Ask for Java location on terminal\>\> where java

-   Download Firefox [here](https://www.mozilla.org/es-ES/firefox/new/)

Run the following code for attaching JAVA

```{r attach Java}
#Sys.getenv("JAVA_HOME")
#Sys.setenv(JAVA_HOME = "C:/Program Files (x86)/Common Files/Oracle/ava/javapath/java.exe") 
#Sys.getenv("PATH")
#install.packages("RSelenium", dependencies=TRUE)


```

Now quit and re star R before running the the next chunk of code.

As described before, Selenium tries to mimic a human. This literally means that we need to open a browser. In RSelenium we do that with the rsDriver function. This function initializes the browser and opens it for us:

A port is just a number. they represent channels through which the computer communicates. When we deploy a docker image, we assign it a port to 'talk' through.

The output the following code will gives is just telling us which drivers it is currently downloading.

```{r set up Selenium browser}
library(RSelenium)

remDr <- rsDriver(port = 4446L, 
                  #If port is taken just try dif. value
                  browser = "firefox",
                  verbose=F,
                  chromever=NULL)

```

`RSelenium` opened a browser which you **don't need to touch**. Everything you do on this browser will be performed using the object `remDr`, where the browser was initiated.`remDr` has a property called `client` from where you can `navigate` to a website.

```{r navigating the link}
remDr$client$navigate(sider_link)


```

## Navigating our site

We will use `client` a lot, so we will save it in our `driver` object.

```{r}
driver <- remDr$client

driver$findElement(value = "(//*[@id='toggleExpand'])")$clickElement()


#Then we get the source code of the web into an HTML string
page_source <- driver$getPageSource()[[1]]

```

## Some more pre scrapping: building URLs for each drug

With the HTML code as a string, we're on familiar ground. We can use read_html to read it.

Note that this is the code for a drug. All other drugs in the webpage will follow under the same structure.

| `<a class="dynatree-title" onclick="gotoDrug('1972')" onmouseover="popDrugNfo('Amphotericin B')" onmouseout="cClick()" href="#">Amphotericin B</a>`

From this html, we are interesting in fetching the numeric ID of each drug. In the example above this is 1972.

```{r}
# Define a regular expression pattern to extract information
pattern <- '<a class="dynatree-title" onclick="gotoDrug\\(\'(\\d+)\'\\)" onmouseover="popDrugNfo\\(\'([^\']+)\'\\)" onmouseout="cClick\\(\\)" href="#">([^<]+)</a>'

# Find all matches in the HTML source code
matches <- str_match_all(page_source, pattern)

# Create a data frame from the matches
matches <- data.frame(matches)

matches <- matches |> 
  rename(hmtl = X1,
         drug_id = X2,
         drug_name = X3 ) |> 
  dplyr::select(-X4) |> 
  distinct(drug_id, .keep_all = T) 

```

```{r}
# Build the links for each medication
drug_links <- paste0("http://sideeffects.embl.de/drugs/", matches$drug_id, "/")

# Adding drug_links to matches
matches <- matches |> 
  mutate(link = drug_links)
```

## Scraping

### Testing

We start by running a couple of tests to see if it works.

```{r scraping a table. ex1}

link1 <- "http://sideeffects.embl.de/drugs/42113/"

# Get all tables from website
table <- link1 |> 
  read_html() |> 
  html_table()

# Extract table 1: side effects
desflurane_s <-  table[[1]] |> 
  dplyr::select(`Side effect`, `Data for drug`) |> 
  clean_names() |> 
  filter(!grepl("postmarketing", data_for_drug, ignore.case = TRUE)) |> 
  filter(across(everything(), ~ !is.na(.) & !(. == ""))) |> 
  mutate(drug_name = "desflurane")

# Extract table 1: indications
desflurane_i <- tibble(table[[2]][[1]]) |> 
  set_names("indication") |> 
  clean_names() |> 
  filter(across(everything(), ~ !is.na(.) & !(. == ""))) |> 
  mutate(drug_name = "desflurane") 

```

In the following test we delete `filter(across(everything(), ~ !is.na(.) & !(. == "")))` as it returns a table with 0 observations and we are interested in the side effects even if they don't have associated `data_for_drug` values. OR NOT???

```{r scraping a table. ex2}

link2 <- "http://sideeffects.embl.de/drugs/2812/"

# Get all tables from website
table2 <- link2 |> 
  read_html() |> 
  html_table()

# Extract table 1: side effects
clotrimazol_s <-  table2[[1]] |> 
  dplyr::select(`Side effect`, `Data for drug`) |> 
  clean_names() |> 
  filter(!grepl("postmarketing", data_for_drug, ignore.case = TRUE)) |> 
  # filter(across(everything(), ~ !is.na(.) & !(. == ""))) |> 
  mutate(drug_name = "amphotericin")

# Extract table 1: indications
clotrimazol_s <- tibble(table2[[2]][[1]]) |> 
  set_names("indication") |> 
   clean_names() %>%
  filter(across(everything(), ~ !is.na(.) & !(. == ""))) %>%
  mutate(drug_name = "desflurane") 
```

Then we create a loop and test it in a random sample of 100 links. In here we also delete the line of code mentioned before.

```{r}
# Obtain a random sample of 100 unique links
sampled_indices <- sample(nrow(matches), 100, replace = FALSE)
sampled_matches <- matches[sampled_indices, ]

# Create lists to store the results
side_effects_tablesS <- list()
indications_tablesS <- list()

# Iterate over each row of the "sampled_matches" table
for (i in seq_len(nrow(sampled_matches))) {
  # Get the medication name and link from the current row
  drug_name <- sampled_matches$drug_name[i]
  link <- sampled_matches$link[i]
  
  # Read and process the link for side effects
  table <- link |> 
    read_html() |> 
    html_table()
  side_effects <-  table[[1]] |> 
    dplyr::select(`Side effect`, `Data for drug`) |> 
    clean_names() |> 
    filter(!grepl("postmarketing", data_for_drug, ignore.case = TRUE)) |> 
    # filter(across(everything(), ~ !is.na(.) & !(. == ""))) |> 
    mutate(drug_name = drug_name)
  
  # Store the result in the side effects tables list
  side_effects_tables[[i]] <- side_effects
  
  # Read and process the link for indications
  table <- link |> 
    read_html() |> 
    html_table()
  indications <- tibble(table[[2]][[1]]) |> 
    set_names("indication") |> 
    clean_names() %>%
    filter(across(everything(), ~ !is.na(.) & !(. == ""))) %>%
    mutate(drug_name = drug_name) 
  
  # Store the result in the indications tables list
  indications_tables[[i]] <- indications
}

  # Assign names to the tables. 
    # WE WOULD ONLY NEED THIS IF WE WANT TO KEEP SEPARATE TABLES FOR EACH DRUG
  # names(side_effects_tablesS) <- paste0(matches$drug_name, "_s")
  # names(indications_tablesS) <- paste0(matches$drug_name, "_i")


# Convert the "data_for_drug" column to character in all tables of side_effects_tablesS
side_effects_tablesS <- lapply(side_effects_tables, function(tbl) {
  tbl$data_for_drug <- as.character(tbl$data_for_drug)
  return(tbl)
})

# Combine all tables of side effects into a single table
side_effectsS <- bind_rows(side_effects_tables)

# Combine all tables of indications into a single table
indicationsS <- bind_rows(indications_tables)

```

### Actual scraping

Finally, here is the code for scraping all the links.

```{r}
# Create lists to store the results
side_effects_tables <- list()
indications_tables <- list()

# Iterate over each row of the "matches" table
for (i in seq_len(nrow(matches))) {
  # Get the medication name and link from the current row
  drug_name <- matches$drug_name[i]
  link <- matches$link[i]
  
  # Read and process the link for side effects
  table <- link |> 
    read_html() |> 
    html_table()
  side_effects <-  table[[1]] |> 
    dplyr::select(`Side effect`, `Data for drug`) |> 
    clean_names() |> 
    filter(!grepl("postmarketing", data_for_drug, ignore.case = TRUE)) |> 
   # filter(across(everything(), ~ !is.na(.) & !(. == ""))) |> ????
    mutate(drug_name = drug_name)
  
  # Store the result in the side effects tables list
  side_effects_tables[[i]] <- side_effects
  
  # Read and process the link for indications
  table <- link |> 
    read_html() |> 
    html_table()
  indications <- tibble(table[[2]][[1]]) |> 
    set_names("indication") |> 
    clean_names() %>%
    filter(across(everything(), ~ !is.na(.) & !(. == ""))) %>%
    mutate(drug_name = drug_name) 
  
  # Store the result in the indications tables list
  indications_tables[[i]] <- indications
}

  # Assign names to the tables
      # WE WOULD ONLY NEED THIS IF WE WANT TO KEEP SEPARATE TABLES FOR EACH DRUG
  # names(side_effects_tables) <- paste0(matches$drug_name, "_s")
  # names(indications_tables) <- paste0(matches$drug_name, "_i")


# Convert the "data_for_drug" column to character in all tables of side_effects_tables
side_effects_tables <- lapply(side_effects_tables, function(tbl) {
  tbl$data_for_drug <- as.character(tbl$data_for_drug)
  return(tbl)
})

# Combine all tables of side effects into a single table
side_effects <- bind_rows(side_effects_tables)

# Combine all tables of indications into a single table
indications <- bind_rows(indications_tables)

```
