---
title: "Data Harvesting final"
format: html
editor: visual
---

pakcages

```{r echo=FALSE}
#install.packages("scrapex")
#install.packages("magrittr")
library(scrapex)
library(xml2)
library(magrittr)
library(lubridate)
library(dplyr)
library(tidyr)
library(httr)
library(rvest)
library(janitor)


```

```{r}
sider_link<- "http://sideeffects.embl.de/drugs/"
```

## Setting up Selenium browser

To do before

-   Download Java [here](https://www.oracle.com/java/technologies/downloads/)

-   Ask for Java location on terminal\>\> where java

-   Download Firefox [here](https://www.mozilla.org/es-ES/firefox/new/)

Run the following code for attaching JAVA

```{r attach Java}
#Sys.getenv("JAVA_HOME")
#Sys.setenv(JAVA_HOME = "C:/Program Files (x86)/Common Files/Oracle/ava/javapath/java.exe") 
#Sys.getenv("PATH")
#install.packages("RSelenium", dependencies=TRUE)


```

Now quit and re star R before running the the next chunk of code.

As described before, Selenium tries to mimic a human. This literally means that we need to open a browser. In RSelenium we do that with the rsDriver function. This function initializes the browser and opens it for us:

```{r set up Selenium browser}
library(RSelenium)

remDr <- rsDriver(port = 4445L, 
                  #If port is taken just try dif. value
                  browser = "firefox",
                  verbose=F,
                  chromever=NULL)

```

`RSelenium` opened a browser which you **don\'t need to touch**. Everything you do on this browser will be performed using the object `remDr`, where the browser was initiated. `remDr` has a property called `client` from where you can `navigate` to a website.

```{r navigating the link}
remDr$client$navigate(sider_link)

```

## Navigating our site


```{r}

driver <- remDr$client

sider_link |> 
  read_html() |> 
  xml_find_all("")

driver$findElement(value = "")$clickElement()


```


## Scraping itself

Here I'm trying the link for Desflurane (N01AB)

```{r scraping a table}

link1<- "http://sideeffects.embl.de/drugs/42113/"

#Get all tables from website
table<- link1 |> 
  read_html() |> 
  html_table()

#Extract only table 1, as it is the one with the data we want.
desflurane <-  table[[1]] |> 
  dplyr::select(`Side effect`, `Data for drug`) |> 
  clean_names() 
```
