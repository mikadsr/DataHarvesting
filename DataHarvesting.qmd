---
title: "Data Harvesting final"
format: html
editor: visual
---

We start by uploading the packages we are going to need

```{r echo=FALSE}
library(scrapex)
library(xml2)
library(magrittr)
library(lubridate)
library(dplyr)
library(tidyr)
library(httr)
library(rvest)
library(janitor)
library(stringr)
library(tidyverse)

```

From now on, `sider_link` is the name we have assigned to the general page we are scraping the data from. However, mirar como explico esto xq no idea.

```{r}
sider_link<- "http://sideeffects.embl.de/drugs/"
```

docker pull selenium/standalone-firefox:2.53.0 sido docker -d -p 4445:4444 selenium/standalone-firefox:2.53.0

## Setting up Selenium browser

To do before

-   Download Java [here](https://www.oracle.com/java/technologies/downloads/)

-   Ask for Java location on terminal\>\> where java

-   Download Firefox [here](https://www.mozilla.org/es-ES/firefox/new/)

Run the following code for attaching JAVA

```{r attach Java}
#Sys.getenv("JAVA_HOME")
#Sys.setenv(JAVA_HOME = "C:/Program Files (x86)/Common Files/Oracle/ava/javapath/java.exe") 
#Sys.getenv("PATH")
#install.packages("RSelenium", dependencies=TRUE)
```

Now quit and re star R before running the the next chunk of code.

As described before, Selenium tries to mimic a human. This literally means that we need to open a browser. In RSelenium we do that with the rsDriver function. This function initializes the browser and opens it for us:

A port is just a number. they represent channels through which the computer communicates. When we deploy a docker image, we assign it a port to 'talk' through.

The output the following code will gives is just telling us which drivers it is currently downloading.

```{r set up Selenium browser}
library(RSelenium)

remDr <- rsDriver(port = 4446L, 
                  #If port is taken just try dif. value
                  browser = "firefox",
                  verbose=F,
                  chromever=NULL)

```

`RSelenium` opened a browser which you **don't need to touch**. Everything you do on this browser will be performed using the object `remDr`, where the browser was initiated.`remDr` has a property called `client` from where you can `navigate` to a website.

```{r navigating the link}
remDr$client$navigate(sider_link)

```

## Navigating our site

We will use `client` a lot, so we will save it in our `driver` object.

```{r}
driver <- remDr$client

driver$findElement(value = "(//*[@id='toggleExpand'])")$clickElement()


#Then we get the source code of the web into an HTML string
page_source <- driver$getPageSource()[[1]]

```

## Some more pre scrapping: building URLs for each drug

With the HTML code as a string, we're on familiar ground. We can use read_html to read it.

Note that this is the code for a drug. All other drugs in the webpage will follow under the same structure.

| `<a class="dynatree-title" onclick="gotoDrug('1972')" onmouseover="popDrugNfo('Amphotericin B')" onmouseout="cClick()" href="#">Amphotericin B</a>`

From this html, we are interesting in fetching the numeric ID of each drug. In the example above this is 1972.

```{r}
# Define a regular expression pattern to extract information
pattern <- '<a class="dynatree-title" onclick="gotoDrug\\(\'(\\d+)\'\\)" onmouseover="popDrugNfo\\(\'([^\']+)\'\\)" onmouseout="cClick\\(\\)" href="#">([^<]+)</a>'

# Find all matches in the HTML source code
matches <- str_match_all(page_source, pattern)

# Create a data frame from the matches
matches <- data.frame(matches)

matches <- matches |> 
  rename(hmtl = X1,
         drug_id = X2,
         drug_name = X3 ) |> 
  dplyr::select(-X4) |> 
  distinct(drug_id, .keep_all = T) 

```

```{r}
# Build the links for each medication
drug_links <- paste0("http://sideeffects.embl.de/drugs/", matches$drug_id, "/")

# Adding drug_links to matches
matches <- matches |> 
  mutate(link = drug_links)
```

## Scraping

### Testing

We start by running a couple of tests to see if it works.

```{r scraping a table. test 1}

link1 <- "http://sideeffects.embl.de/drugs/42113/"

# Get all tables from website
table <- link1 |> 
  read_html() |> 
  html_table()

# Extract table 1: side effects
desflurane_s <-  table[[1]] |> 
  dplyr::select(`Side effect`, `Data for drug`) |> 
  clean_names() |> 
  filter(!grepl("postmarketing", data_for_drug, ignore.case = TRUE)) |> 
  filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
  mutate(drug_name = "desflurane")

# Extract table 1: indications
desflurane_i <- tibble(table[[2]][[1]]) |> 
  set_names("indication") |> 
  clean_names() |> 
  filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
  mutate(drug_name = "desflurane") 

```

In the following test we delete `filter(across(everything(), ~ !is.na(.) & !(. == "")))` as it returns a table with 0 observations and we are interested in the side effects even if they don't have associated `data_for_drug` values. OR NOT???

```{r scraping a table. test 2}

link2 <- "http://sideeffects.embl.de/drugs/2812/"

# Get all tables from website
table2 <- link2 |> 
  read_html() |> 
  html_table()

# Extract table 1: side effects
clotrimazol_s <-  table2[[1]] |> 
  dplyr::select(`Side effect`, `Data for drug`) |> 
  clean_names() |> 
  filter(!grepl("postmarketing", data_for_drug, ignore.case = TRUE)) |> 
  filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
  mutate(drug_name = "clotrimazol")

# Extract table 1: indications
clotrimazol_i <- tibble(table2[[2]][[1]]) |> 
  set_names("indication") |> 
   clean_names() |> 
  filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
  mutate(drug_name = "clotrimazol") 

```

Then we create a loop and test it in a random sample of 100 links.

```{r scraping a sample. test3}
# Obtain a random sample of 100 unique links
sampled_indices <- sample(nrow(matches), 100, replace = FALSE)
sampled_matches <- matches[sampled_indices, ]

# Create lists to store the results
side_effects_tablesS <- list()
indications_tablesS <- list()

# Iterate over each row of the "sampled_matches" table
for (i in seq_len(nrow(sampled_matches))) {
  # Get the medication name and link from the current row
  drug_name <- sampled_matches$drug_name[i]
  link <- sampled_matches$link[i]
  
  # Read and process the link for side effects
  table <- link |> 
    read_html() |> 
    html_table()
  side_effectsS <-  table[[1]] |> 
    dplyr::select(`Side effect`, `Data for drug`) |> 
    clean_names() |> 
    filter(!grepl("postmarketing", data_for_drug, ignore.case = TRUE)) |> 
    filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
    mutate(drug_name = drug_name)
  
  # Store the result in the side effects tables list
  side_effects_tablesS[[i]] <- side_effectsS
  
  # Read and process the link for indications
  table <- link |> 
    read_html() |> 
    html_table()
  indicationsS <- tibble(table[[2]][[1]]) |> 
    set_names("indication") |> 
    clean_names() |> 
    filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
    mutate(drug_name = drug_name) 
  
  # Store the result in the indications tables list
  indications_tablesS[[i]] <- indicationsS
}

# Convert the "data_for_drug" column to character in all tables of side_effects_tablesS
side_effects_tablesS <- lapply(side_effects_tablesS, function(tbl) {
  tbl$data_for_drug <- as.character(tbl$data_for_drug)
  return(tbl)
})

# Combine all tables of side effects into a single table
side_effectsS <- bind_rows(side_effects_tablesS)

# Combine all tables of indications into a single table
indicationsS <- bind_rows(indications_tablesS)

```

### Actual scraping

Finally, here is the code for scraping all the links.

```{r final scraping process}
# Create lists to store the results
side_effects_tables <- list()
indications_tables <- list()

# Iterate over each row of the "matches" table
for (i in seq_len(nrow(matches))) {
  # Get the medication name and link from the current row
  drug_name <- matches$drug_name[i]
  link <- matches$link[i]
  
  # Read and process the link for side effects
  table <- link |> 
    read_html() |> 
    html_table()
  side_effects <-  table[[1]] |> 
    dplyr::select(`Side effect`, `Data for drug`) |> 
    clean_names() |> 
    filter(!grepl("postmarketing", data_for_drug, ignore.case = TRUE)) |> 
    filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
    mutate(drug_name = drug_name)
  
  # Store the result in the side effects tables list
  side_effects_tables[[i]] <- side_effects
  
  # Read and process the link for indications
  table <- link |> 
    read_html() |> 
    html_table()
  indications <- tibble(table[[2]][[1]]) |> 
    set_names("indication") |> 
    clean_names() |> 
    filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
    mutate(drug_name = drug_name) 
  
  # Store the result in the indications tables list
  indications_tables[[i]] <- indications
}

# Convert the "data_for_drug" column to character in all tables of side_effects_tables
side_effects_tables <- lapply(side_effects_tables, function(tbl) {
  tbl$data_for_drug <- as.character(tbl$data_for_drug)
  return(tbl)
})

# Combine all tables of side effects into a single table
side_effects <- bind_rows(side_effects_tables)

# Combine all tables of indications into a single table
indications <- bind_rows(indications_tables)

```

### Getting ATC code

```{r getting atc codes. test1}
link2 <- "http://sideeffects.embl.de/drugs/1972/"

atc_codes<- link2 |>
  read_html() |>
  html_nodes("strong:contains('ATC Code') + a") |>
  html_text(trim = TRUE)
```

now that we figured out the code, we can store it in a function for its easy reuse and apply it to our sampled_matches dataframe.

```{r function for getting atc codes}
extrct_ATC_codes  <- function(url) {
  result <- url |> 
    read_html() |> 
    html_nodes("strong:contains('ATC Code') + a") |> 
    html_text(trim = TRUE)

  return(result)
}

sampled_matches$atc_codes <- lapply(sampled_matches$link,
                       extrct_ATC_codes)

```

we just want to keep the first letter of the ATC code since is the biggest category and what we are actually interested in. Bare in mind that the OCDE data gathers information for all of this big categories but not for the subcategories, which could lead to biases in our analysis. no sé pensar mejor esto.

```{r}
sampled_matches <- sampled_matches |> 
  mutate(atc_codes = substr(atc_codes, 1, 1))

sampled_matches

```

## Recodification of side_effects

| Frequency class | Intervention cohort- clinical trials frequency | Assigned value |
|------------------|------------------------------------|------------------|
| very frequent   | More than 10%                                  | 5              |
| frequent        | 1 to 10%                                       | 4              |
| infrequent      | 0.1 to 1%                                      | 3              |
| rare            | 0.01 to 0.1%                                   | 2              |
| very rare       | Less than 0.01%                                | 1              |
| zeros\*         | 0%                                             | 0              |

incluir en la explicación del código: If the max value is NA, it takes the min value. Otherwise, it calculates the average of min and max, and then multiplies the result by 100.

```{r ass_values to numbers}
filtered_rows <- side_effectsS |> 
  # Filter rows based on the presence of '-' or '%' in data_for_drug and absence of ','
  filter(grepl("-|%", data_for_drug) & !grepl(",", data_for_drug)) |> 
  # Remove '%' from data_for_drug column
  mutate(data_for_drug = gsub("%", "", data_for_drug)) |> 
  # Split data_for_drug into min and max columns
  separate(data_for_drug, into = c("min", "max"), sep = "-") |> 
  # Convert min and max columns to numeric and express as decimals
  mutate(across(c(min, max), ~as.numeric(.)*0.01)) |> 
  # Calculate occu_freq as mean of min and max, handling NA values
  mutate(occu_freq = ifelse(is.na(max), min,
                            ifelse(is.na(min), max,
                                   (min + max) / 2)) * 100 / 5) |> 
  # Calculate ass_value based on specified ranges
  mutate(ass_value = case_when(
    occu_freq > 2 ~ 5,
    occu_freq >= 0.2 ~ 4,
    occu_freq >= 0.02 ~ 2,
    occu_freq >= 0.002 ~ 1,
    occu_freq == 0 ~ 0,
    TRUE ~ NA_real_
  )) |> 
  # Remove min and max columns
  select(-c(min, max, occu_freq))

```

```{r ass_values to words}
# Function to extract ONLY words
extract_words <- function(text) {
  str_extract_all(text, "(?<!\\d)[a-zA-Z]+(?:\\s+[a-zA-Z]+)*") |> 
    map_chr(~ ifelse(length(.x) > 0, paste(.x, collapse = ", "), NA_character_))
}

# Apply the function to the data_for_drug column
filtered_rows2 <- side_effectsS |> 
  mutate(words_only = extract_words(data_for_drug)) |> 
  # Filter out rows with NA values or containing the word "to" in words_only 
  filter(!is.na(words_only), 
         words_only != "to") |> 
  # Splitting the words before and after the comma into two new columns
  separate(words_only, into = c("words_only", "after_comma"), 
           sep = ",", 
           remove = FALSE) |> 
  # Removing any additional whitespace around the words after the comma
  mutate(across(c(after_comma), trimws),
         # Assigning numerical values to the words in both variables
         across(c(words_only, after_comma), 
                ~ case_when(
                  . %in% c("very frequent", "very common") ~ 5,
                  . %in% c("frequent", "common") ~ 4,
                  . == "infrequent" ~ 3,
                  . %in% c("rare", "uncommon") ~ 2,
                  . %in% c("very rare", "very uncommon") ~ 1,
                  TRUE ~ NA_real_)),
         # Calculating the mean value between "words_only_value" and "after_comma_value"
         mean_value = (words_only + after_comma) / 2,
         # Assigning values to the "ass_value" column based on specified conditions
         ass_value = ifelse(is.na(mean_value), words_only, ceiling(mean_value))) |> 
  # Deleting the columns we no longer need
  select(-c(words_only, after_comma, mean_value, data_for_drug))

```

Finally, we combine the new datasets:

```{r}
sampled_side_effects <- bind_rows(filtered_rows, filtered_rows2)

sampled_side_effects <- merge(sampled_matches, sampled_side_effects, by = "drug_name", all = TRUE)

sampled_side_effects <- sampled_side_effects |> 
  na.omit(ass_values) |> 
  select(-c(hmtl, link, drug_id)) |> 
  arrange(atc_codes)

```

## OCDE data

OCDE Health Statistics database includes data on total pharmaceutical consumption according to the Anatomical Therapeutic Chemical (ATC) classification/Defined Daily Dose (DDD) system, created by the WHO Collaborating Centre for Drug Statistics Methodology.

The ATC classification system divides drugs into different groups according to the organ system on which they act and/or therapeutical, pharmacological and chemical characteristics:

|                                                                       |                        |
|------------------------------------------|------------------------------|
| **Main groups**                                                       | **Codes (2023 Index)** |
| A-Alimentary tract and metabolism                                     | A                      |
| B-Blood and blood forming organs                                      | B                      |
| C-Cardiovascular system                                               | C                      |
| G-Genito urinary system and sex hormones                              | G                      |
| H-Systemic hormonal preparations, excluding sex hormones and insulins | H                      |
| J-Antiinfectives for systemic use                                     | J                      |
| M-Musculo-skeletal system                                             | M                      |
| N-Nervous system Analgesics Anxiolytics                               | N                      |
| R-Respiratory system                                                  | R                      |

We are working with the data from the most recent year available. The unit of measure if defined daily dosage per 1.000 inhabitants per day

```{r}
ocde <- read.csv("HEALTH_PHMC.csv")

ocde <- ocde |> 
  select(-c(VAR, UNIT, Measure, YEA, Flag.Codes, Flags)) |> 
  filter(Year == 2021) |> 
  rename(ISO = COU) |> 
  separate(Variable, 
           into = c("Varcode", "Variable"), 
           sep = "-", 
           extra = "merge") |> 
  filter(nchar(Varcode) == 1) |> 
  clean_names()
```
