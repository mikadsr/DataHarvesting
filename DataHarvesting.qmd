---
title: "Data Harvesting final"
format: html
editor: visual
---

pakcages

```{r echo=FALSE}
#install.packages("scrapex")
#install.packages("magrittr")
library(scrapex)
library(xml2)
library(magrittr)
library(lubridate)
library(dplyr)
library(tidyr)
library(httr)
library(rvest)
library(janitor)
library(stringr)
library(stringr)
library(tidyverse)


```

```{r}
sider_link<- "http://sideeffects.embl.de/drugs/"
```

docker pull selenium/standalone-firefox:2.53.0 sido docker -d -p 4445:4444 selenium/standalone-firefox:2.53.0

## Setting up Selenium browser

To do before

-   Download Java [here](https://www.oracle.com/java/technologies/downloads/)

-   Ask for Java location on terminal\>\> where java

-   Download Firefox [here](https://www.mozilla.org/es-ES/firefox/new/)

Run the following code for attaching JAVA

```{r attach Java}
#Sys.getenv("JAVA_HOME")
#Sys.setenv(JAVA_HOME = "C:/Program Files (x86)/Common Files/Oracle/ava/javapath/java.exe") 
#Sys.getenv("PATH")
#install.packages("RSelenium", dependencies=TRUE)


```

Now quit and re star R before running the the next chunk of code.

As described before, Selenium tries to mimic a human. This literally means that we need to open a browser. In RSelenium we do that with the rsDriver function. This function initializes the browser and opens it for us:

A port is just a number. they represent channels through which the computer communicates. When we deploy a docker image, we assign it a port to 'talk' through.

The output the following code will gives is just telling us which drivers it is currently downloading.

```{r set up Selenium browser}
library(RSelenium)

remDr <- rsDriver(port = 4446L, 
                  #If port is taken just try dif. value
                  browser = "firefox",
                  verbose=F,
                  chromever=NULL)

```

`RSelenium` opened a browser which you **don't need to touch**. Everything you do on this browser will be performed using the object `remDr`, where the browser was initiated.`remDr` has a property called `client` from where you can `navigate` to a website.

```{r navigating the link}
remDr$client$navigate(sider_link)


```

## Navigating our site

We will use `client` a lot, so we will save it in our `driver` object.

```{r}
driver <- remDr$client

driver$findElement(value = "(//*[@id='toggleExpand'])")$clickElement()


#Then we get the source code of the web into an HTML string
page_source <- driver$getPageSource()[[1]]

```

## Some more pre scrapping: building URLs for each drug

With the HTML code as a string, we're on familiar ground. We can use read_html to read it.

Note that this is the code for a drug. All other drugs in the webpage will follow under the same structure.

| `<a class="dynatree-title" onclick="gotoDrug('1972')" onmouseover="popDrugNfo('Amphotericin B')" onmouseout="cClick()" href="#">Amphotericin B</a>`

From this html, we are interesting in fetching the numeric ID of each drug. In the example above this is 1972.

```{r}
# Define a regular expression pattern to extract information
pattern <- '<a class="dynatree-title" onclick="gotoDrug\\(\'(\\d+)\'\\)" onmouseover="popDrugNfo\\(\'([^\']+)\'\\)" onmouseout="cClick\\(\\)" href="#">([^<]+)</a>'

# Find all matches in the HTML source code
matches <- str_match_all(page_source, pattern)

# Create a data frame from the matches
matches <- data.frame(matches)

matches <- matches |> 
  rename(hmtl = X1,
         drug_id = X2,
         drug_name = X3 ) |> 
  dplyr::select(-X4)

```

## Scraping

Here I'm trying the link for Desflurane (N01AB)

```{r scraping a table}

link1<- "http://sideeffects.embl.de/drugs/42113/"

#Get all tables from website
table<- link1 |> 
  read_html() |> 
  html_table()

#Extract only table 1, as it is the one with the data we want.
desflurane <-  table[[1]] |> 
  dplyr::select(`Side effect`, `Data for drug`) |> 
  clean_names() |> 
  filter(!grepl("postmarketing", data_for_drug, ignore.case = TRUE)) |> 
  filter(across(everything(), ~ !is.na(.) & !(. == "")))


desflurane 

```
