---
title: "Data Harvesting final"
authors: "Sara Cristina Herranz Amado, Mikaela de Smedt Ronquillo"
date: '13/03/2024'
format: html
editor: visual
---

## Introduction

In the culmination of our Data Harvesting course, our focus turns to the landscape of pharmaceuticals. Our project aims to provide a descriptive analysis of drug side effects and indications across various pharmaceuticals and draw connections to OECD countries, grounded in the consumption patterns of distinct drug classes.

Our primary data source is SIDER, a website that meticulously compiles information on marketed medicines and their associated adverse drug reactions. Extracted from public documents and package inserts, SIDER furnishes us with essential details such as side effect frequencies, drug classifications, and relevant links for further exploration, including drug--target relations.

By using the OECD's open data portal, we aim to enrich our analysis by incorporating data on pharmaceutical consumption. This amalgamation of SIDER and OECD datasets is anticipated to give us insights into the prevalence of side effects, whilst making this information more accessible.

To navigate the web-based data retrieval process, we employ web scraping techniques and leverage RSelenium in order to interact with different elements in the website that will allow us to access the data. Subsequently, we engage in meticulous data wrangling to refine our raw data, preparing it for clear and concise visualizations.

Our project aspires to offer straightforward access to pharmaceutical information, whilst also allowing us to put our data harvesting skills to the test.

```{r echo=FALSE}
library(scrapex)
library(xml2)
library(magrittr)
library(lubridate)
library(dplyr)
library(tidyr)
library(httr)
library(rvest)
library(janitor)
library(stringr)
library(tidyverse)

```

### The very first step

We know the web we want to get data from. So we must define this within our programing environment. From now on, `sider_link` is the name we have assigned to the general page we are scraping the data from.

```{r}
sider_link<- "http://sideeffects.embl.de/drugs/"
```

## Setting up Selenium browser

Selenium is a powerful automation tool that can be used for web scraping tasks. By automating interactions with web pages, Selenium can navigate through complex web structures, fill out forms, click buttons, and extract desired information from HTML elements. Selenium supports multiple programming languages such as Java, Python, JavaScript, and R. To make the most of its capabilities, it is important to complete these steps first:

1.  Download and install Java from the official website.

2.  Use the terminal or command prompt to determine the location of Java by executing the command: **`where java`**.

3.  Download and install the Firefox web browser from the official website.

4.  Run the following code to configure RSelenium to use Java:

```{r attach Java}
# Sys.getenv("JAVA_HOME")
Sys.setenv(JAVA_HOME = "C:/Program Files (x86)/Common Files/Oracle/ava/javapath/java.exe") 
Sys.getenv("PATH")
install.packages("RSelenium", dependencies=TRUE)
```

Before proceeding with the next code chunk, we quit and restart R.

`RSelenium` simulates what we would do in a webpage by opening a browser. In `RSelenium`, this is accomplished using the `rsDriver` function, which initializes the browser and opens it for us. It does so by using a port, which is a numerical identifier that represent communication channels between computers. When deploying a Docker image, we assign it a port through which it can communicate with other systems.

The output of the following code will indicate the drivers that are currently being downloaded. This is essential for ensuring that the necessary drivers are available for the web browser to function correctly during the web scraping process.

```{r set up Selenium browser}
library(RSelenium)

remDr <- rsDriver(port = 4446L, 
                  #If port is taken just try dif. value
                  browser = "firefox",
                  verbose=F,
                  chromever=NULL)

```

`RSelenium` opened a browser which we don't need to touch, as everything will be performed using the object `remDr`, where the browser was initiated. Moreover, `remDr` has a property called `client` from where we can `navigate` to our website. Since we will be using this property several times, we stored it in an object named `driver`.

```{r navigating the link}
remDr$client$navigate(sider_link)

# Save client in our 'driver' object
driver <- remDr$client
```

### Navigating our site

[SIDER 4.1: Side Effect Resource](http://sideeffects.embl.de/) is a web page that contains information on marketed medicines and their recorded adverse drug reactions. The information is extracted from public documents and package inserts. The available information include side effect frequency, drug and side effect classifications as well as links to further information, for example drug--target relations. The *Drug list* section includes an option to browse drugs by Anatomical Therapeutic Chemical (ATC) classification. It divides drugs into different groups according to the organ system on which they act and/or therapeutical, pharmacological and chemical characteristics. These ATC classifications are organized into dropdown menus, which contain the items of interest.

With the developer's tools (CTRL+SHIFT+C or CMD+SHIFT+C for Mac users), we can access the HTML code behind the website. We are interested in extracting all the drugs associated with the different ATC categories. However, unless we click on the drop-down menu (called 'Expand all nodes'), those drugs won't be displayed in the source code of the website, and we would not have access to the variables we are actually interested in. This is the main reason we use `RSelenium`: since our website is interactive, in order to scrape data we need a software program that will click on required places.

![This is the button in question. To the right, we find how its identified within the Web's HTML and see its Xpath: //\*[@id="toggleExpand"]](expand%20nodes%20button.png)

After examining the HTML code and finding the XPath expression for the 'Expand all nodes' button, we are able to locate it in `RSelenium` and click on it. We use our driver object with `findElement` (to position the browser pointer exactly where we want) and `clickElement` (to click on said element). Once all the information is displayed in the source code of the website after using `getPageSource`, we save it into an HTML string that will be later used for the scraping process.

```{r getting the source code}
# Ask Selenium to click on the expand all button
driver$findElement(value = "(//*[@id='toggleExpand'])")$clickElement()

# Save the source code of the web into an HTML string
page_source <- driver$getPageSource()[[1]]

```

## Some more pre scraping: building URLs for each drug

However, even after using `RSelenium` to click on the drop down menu, we still don't have the information necessary to access the URL for each drug. To do so, we first need to find a pattern in the HTML code that give as information about this.

When we initially explored the SIDER page, we noticed a consistent pattern in the URLs for each drug: following the general link of the page, there was a unique number or code that corresponded to each drug. We regarded this number the numeric ID of the drug. For instance, the link for the drug Amphotericin B was <http://sideeffects.embl.de/drugs/1972/>, with '1972' representing its ID. This ID is solely present in the URL for each drug, as there is no available list to reference all of them. Since our goal is to scrap data for each drug, we need to access each individual link and, consequently, identify their IDs to build all the links.

![Example of HTML behind Amphotericin B showing its unique code](showing%20the%20structure%20within%20the%20tree.png)

We have previously parsed the HTML code of the website as a string, so we can now use `read_html` to process it. But we still need to know what to look for so we can tell R how to fetch it for us.

### Breaking the HTML down

As seen in the picture above, the elements we are interested in getting are found in the following HTML chunk which a hyperlink (**`<a>`** element) with specific attributes and inline event handlers

```{=html}
<a class="dynatree-title" onclick="gotoDrug('1972')" onmouseover="popDrugNfo('Amphotericin B')" onmouseout="cClick()" href="#">Amphotericin B</a>
```
Lets break it down even further...

1.  **`<a>` element**: This is an HTML anchor (link) element.

2.  **`class="dynatree-title"`**: The **`class`** attribute assigns the CSS class "dynatree-title" to the link. This class can be used for styling purposes, and it might be associated with a specific look or behavior in the user interface.

3.  **`onclick="gotoDrug('1972')"`**: is an inline event handler that specifies that when the link is clicked, the function **`gotoDrug`** should be called with the argument '1972'.

4.  **`onmouseover="popDrugNfo('Amphotericin B')"`**: Another inline event handler, this time for the **`onmouseover`** event, which triggers when the mouse hovers over the link. It calls the function **`popDrugNfo`** with 'Amphotericin B' as the argument. This function handles displaying additional information (pop-up) about the drug when the mouse is over the link.

5.  **`onmouseout="cClick()"`**: This event handler triggers when the mouse leaves the link. It calls the **`cClick`** function.

6.  **`href="#"`**: This attribute is set to "\#", indicating a hyperlink that doesn't navigate anywhere. This is used as a placeholder or for JavaScript-based interactions rather than navigating to a new page.

7.  **Content between `<a>` tags**: The text "Amphotericin B" is the visible content of the hyperlink. This is what users see and click on.

We can see now that the drug name and drug code we are interested in are found in elements listed above as 4 and 3 respectively. To access them, we need to use the XPath.

The XPath below represents the code for a drug, Amphotericin B. However, we observed that all other drugs in the webpage followed under the same structure:

| `<a class="dynatree-title" onclick="gotoDrug('1972')" onmouseover="popDrugNfo('Amphotericin B')" onmouseout="cClick()" href="#">Amphotericin B</a>`

As seen above, `onclick="gotoDrug('1972')` corresponds to the numeric ID of the drug, while `onmouseover="popDrugNfo('Amphotericin B')` refers to its name. After identifying this pattern, we defined a regular expression named "pattern" to extract this information from the HTML source code using `str_match_all`. Then, we constructed a data frame named "matches", which included the following details:

-   'html': the HTML pattern.

-   'drug_id': the numeric ID.

-   'drug_name': the drug name.

```{r finding patterns}
# Define a regular expression pattern to extract information
pattern <- '<a class="dynatree-title" onclick="gotoDrug\\(\'(\\d+)\'\\)" onmouseover="popDrugNfo\\(\'([^\']+)\'\\)" onmouseout="cClick\\(\\)" href="#">([^<]+)</a>'

# Find all matches in the HTML source code
matches <- str_match_all(page_source, pattern)

# Create a data frame from the matches
matches <- data.frame(matches) |> 
  rename(html = X1,
         drug_id = X2,
         drug_name = X3) |> 
  dplyr::select(-X4) |> 
  distinct(drug_id, .keep_all = TRUE)

```

After this crucial step, we are finally able to build the links from which we will be scraping the data. To achieve this, we paste together the general link followed by each drug ID available in `matches`, followed by a slash (/). Finally, we add another column to `matches` to store all the links alongside their respective drug names and IDs.

```{r building links}
# Build the links for each medication
drug_links <- paste0("http://sideeffects.embl.de/drugs/", matches$drug_id, "/")

# Add drug_links to matches
matches <- matches |> 
  mutate(link = drug_links) |> 
  select(-html)
```

Having done this, we now have our URLs for all the drugs shown in the website.

```{r flex matches}
head(matches)
```

## Web scraping

### Testing: individual tests

Before proceeding with the code for all drugs, we conduct preliminary tests using two specific drugs: Desflurane and Clotrimazol. The procedure for both drugs is identical. Initially, we retrieve the HTML code for each drug page and extract all available tables. Then, we extract the side effects table, `table[[1]]`, and the indications table, `table[[2]]`. This allows us to ensure that the code functions correctly for individual drugs before scaling it up to a bigger sample.

We start by defining the URL `link1`, which corresponds to Desflurane. `read_html()` retrieves the HTML content of the webpage, from where we extract all the tables present on the webpage by using `html_table()`. This step allows us to capture all the relevant information available in tabular format.

```{r scraping test 1. html}
link1 <- "http://sideeffects.embl.de/drugs/42113/"

table <- link1 |> 
  read_html() |> 
  html_table()

```

After extracting the tables, we access the first table using `table[[1]]`, which contains information about the side effects associated with the drug. We then select only the relevant columns, such as 'Side effect' and 'Data for drug' (which represents the incidence of the corresponding side effect). Additionally, we filter out rows where the 'Data for drug' column contains the term "postmarketing", as these entries may not be directly relevant to the drug's primary effects. Furthermore, we apply a filter using `if_all()` to remove rows where any column contains missing values or empty strings. Finally, we add a new column named 'drug_name' to identify the drug associated with the side effects table, which in this case is "desflurane".

```{r scraping test 1. extracting tables}
# Extract table 1: side effects
desflurane_s <-  table[[1]] |> 
  dplyr::select(`Side effect`, 
                `Data for drug`) |> 
  clean_names() |> 
  filter(!grepl("postmarketing", 
                data_for_drug, 
                ignore.case = TRUE)) |> 
  filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
  mutate(drug_name = "desflurane")

head(desflurane_s)

```

Similarly, we extract the second table from the webpage, which contains information about drug indications. We create a tibble from the second table's first column using `tibble(table[[2]][[1]])`, as the variable 'Indications' is repeated three times and we just want to keep one of those columns. We also filter out rows with missing values or empty strings using `if_all()`. Finally, we add a "drug_name" column to identify the drug associated with the indications table, which is again "desflurane".

```{r scraping test 1. extracting tables}
# Extract table 2: indications
desflurane_i <- tibble(table[[2]][[1]]) |> 
  set_names("indication") |> 
  clean_names() |> 
  filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |>
  mutate(drug_name = "desflurane") 

head(desflurane_i)
```

In conclusion, these chunks of code are designed to systematically extract relevant information about side effects and indications for a specific drug from the SIDER website. Now we can extend this procedure to all the links we previously constructed.

### Testing: creating a random sample

To ensure the accuracy of our scraping process before proceeding with all 1117 drug links, we created a sample dataset. This sample allows us to test our scraping code within a loop, ensuring that it functions correctly. By applying the code to a smaller subset of data first, we can identify any potential issues or errors and make necessary adjustments before running the code on the entire collection of links, minimizing errors and ensuring the reliability of our scraping process.

We start by obtaining a random sample of 100 unique links from the "matches" table. This is achieved by generating a random sample of row indices using the `sample()` function. The `replace = FALSE` argument ensures that each index is selected only once, thus ensuring uniqueness. We then use these sampled indices to subset the "matches" table, resulting in a new data frame called "sampled_matches".

```{r scraping a sample. random sampling}
# Obtain a random sample of 100 unique links
sampled_indices <- sample(nrow(matches), 
                          100, 
                          replace = FALSE)

sampled_matches <- matches[sampled_indices, ]

```

The next step involves creating empty lists to store the results of scraping: one for side effects tables (`side_effects_tablesS`) and another for indications tables (`indications_tables_sample`). These lists will be populated with the scraped data from each link.

```{r scraping a sample. lists}
# Create lists to store the results
side_effects_tables_sample<- list()
indications_tables_sample <- list()

```

We then iterate over each row of the "sampled_matches" table using a for loop. Within each iteration, we extract the drug name and link from the current row. The drug name is used to identify the drug associated with the scraped data, while the link is used to access the webpage containing the information.

For each link, we scrape two tables (side effects and indications) following the process described in the previous section. After processing, both the side effects and indications tables for each drug are stored in their respective lists (`side_effects_tables_sample`and `indications_tables_sample`).

```{r scraping a sample. for loop}
# Iterate over each row of the "sampled_matches" table
for (i in seq_len(nrow(sampled_matches))) {
  # Get the medication name and link from the current row
  drug_name <- sampled_matches$drug_name[i]
  link <- sampled_matches$link[i]
  
  # Read and process the link for side effects
  table <- link |> 
    read_html() |> 
    html_table()
  side_effects_sample <-  table[[1]] |> 
    dplyr::select(`Side effect`, `Data for drug`) |> 
    clean_names() |> 
    filter(!grepl("postmarketing", data_for_drug, ignore.case = TRUE)) |> 
    filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
    mutate(drug_name = drug_name)
  
  # Store the result in the side effects tables list
  side_effects_tables_sample[[i]] <- side_effects_sample
  
  # Read and process the link for indications
  table <- link |> 
    read_html() |> 
    html_table()
  indications_sample <- tibble(table[[2]][[1]]) |> 
    set_names("indication") |> 
    clean_names() |> 
    filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
    mutate(drug_name = drug_name) 
  
  # Store the result in the indications tables list
  indications_tables_sample[[i]] <- indications_sample
}

```

Following the scraping process, we convert the 'data_for_drug' column to character type in all tables of the `side_effects_tables_sample` list to ensure consistency. Finally, we combine all tables of side effects into a single table named, `side_effects_sample`, and similarly, we combine all tables of indications into a single table, `indications_sample`.

```{r scraping a sample. final tables}
# Convert the "data_for_drug" column to character in all tables of side_effects_tablesS
side_effects_tables_sample<- lapply(side_effects_tables_sample, function(tbl) {
  tbl$data_for_drug <- as.character(tbl$data_for_drug)
  return(tbl)
})

# Combine all tables of side effects into a single table
side_effects_sample <- bind_rows(side_effects_tables_sample)

# Combine all tables of indications into a single table
indications_sample <- bind_rows(indications_tables_sample)

```

We can now see our dataframes containing the side effects and indications

```{r some more flexing}
head(side_effects_sample)
head(indications_sample)
```

```{r, echo=FALSE}
rm(side_effects_sample, 
   indications_sample, 
   indications_tables_sample, 
   side_effects_tables_sample)
```

### Actual scraping

Finally, following the successful tests conducted earlier, we proceed with scraping the entire set of links. The process remains consistent with the steps described in the preceding section.

The first step involves creating empty lists to store the results of scraping: one for side effects tables (`side_effects_tables`) and another for indications tables (`indications_tables`). These lists will be populated with the scraped data from each link.

```{r final scraping. lists}
# Create lists to store the results
side_effects_tables <- list()
indications_tables <- list()

```

We then iterate over each row of the "matches" table using a for loop. Within each iteration, we extract the drug name and link from the current row. The drug name is used to identify the drug associated with the scraped data, while the link is used to access the webpage containing the information.

For each link, we scrape two tables (side effects and indications) following the process described in "Testing: individual tests". After processing, both the side effects and indications tables for each drug are stored in their respective lists (`side_effects_tables` and `indications_tables`).

> **NOTE**: This step loops through 1117 links so please expect it to take a while. Further down this .qmd you will get the option of running everything either on the complete scraped data or the sampled data. To run on the sampled data just turn eval = TRUE on the {r} at the top of the chunk.

```{r final scraping. looping through our links}
# Iterate over each row of the "matches" table
for (i in seq_len(nrow(matches))) {
  # Get the medication name and link from the current row
  drug_name <- matches$drug_name[i]
  link <- matches$link[i]
  
  # Read and process the link for side effects
  table <- link |> 
    read_html() |> 
    html_table()
  side_effects <-  table[[1]] |> 
    dplyr::select(`Side effect`, `Data for drug`) |> 
    clean_names() |> 
    filter(!grepl("postmarketing", data_for_drug, ignore.case = TRUE)) |> 
    filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
    mutate(drug_name = drug_name)
  
  # Store the result in the side effects tables list
  side_effects_tables[[i]] <- side_effects
  
  # Read and process the link for indications
  table <- link |> 
    read_html() |> 
    html_table()
  indications <- tibble(table[[2]][[1]]) |> 
    set_names("indication") |> 
    clean_names() |> 
    filter(if_all(everything(), ~ !is.na(.) & !(. == ""))) |> 
    mutate(drug_name = drug_name) 
  
  # Store the result in the indications tables list
  indications_tables[[i]] <- indications
}

```

Following the scraping process, we convert the 'data_for_drug' column to character type in all tables of the `side_effects_tables` list to ensure consistency. Finally, we combine all tables of side effects into a single table named, `side_effects`, and similarly, we combine all tables of indications into a single table, `indications`.

```{r final scraping. for loop}
# Convert the "data_for_drug" column to character in all tables of side_effects_tables
side_effects_tables <- lapply(side_effects_tables, function(tbl) {
  tbl$data_for_drug <- as.character(tbl$data_for_drug)
  return(tbl)
})

# Combine all tables of side effects into a single table
side_effects <- bind_rows(side_effects_tables)

# Combine all tables of indications into a single table
indications <- bind_rows(indications_tables)

```

### Getting ATC codes

While setting the objectives of our assignment, we recognized the potential of leveraging the Anatomical Therapeutic Chemical (ATC) classification for our data analysis objectives. This classification system is present both on the SIDER web page, from which we've scraped data, and in the OCDE dataset we'll be utilizing (which will be described later on). However, during the initial scraping process of SIDER, which primarily focused on retrieving tables, we did not capture these ATC codes. Therefore, we need to come up with a different method to get them.

The pages we access using the URLs we previously built look as follows:

![](getting%20ATC%20codes.png)

The HTML for the codes we are interested in is as follows

```{=html}
<p><strong>ATC Codes</strong>:
      <a href="http://www.whocc.no/atc_ddd_index/?code=A01AB04" data-original-title="" title="">A01AB04</a>,
      <a href="http://www.whocc.no/atc_ddd_index/?code=A07AA07" data-original-title="" title="">A07AA07</a>,
      <a href="http://www.whocc.no/atc_ddd_index/?code=G01AA03" data-original-title="" title="">G01AA03</a>,
      <a href="http://www.whocc.no/atc_ddd_index/?code=J02AA01" data-original-title="" title="">J02AA01</a>
    </p>
```
This HTML represents a paragraph (**`<p>`**) containing a list of links related to ATC codes. Let's break it down:

1.  **`<p>`**: This is the opening tag for a paragraph.

2.  **`<strong>`**: This is a strong (or bold) tag. In this case, it's used to emphasize the text "ATC Codes."

3.  "ATC Codes": This is the text within the **`<strong>`** tag, representing the title or heading of the content.

4.  **`<a>`**: These are anchor tags, which create hyperlinks. Each **`<a>`** tag contains an **`href`** attribute, which specifies the URL to which the link points.

5.  Links:

    -   **`<a href="http://www.whocc.no/atc_ddd_index/?code=A01AB04" data-original-title="" title="">A01AB04</a>`**

    -   **`<a href="http://www.whocc.no/atc_ddd_index/?code=A07AA07" data-original-title="" title="">A07AA07</a>`**

    -   **`<a href="http://www.whocc.no/atc_ddd_index/?code=G01AA03" data-original-title="" title="">G01AA03</a>`**

    -   **`<a href="http://www.whocc.no/atc_ddd_index/?code=J02AA01" data-original-title="" title="">J02AA01</a>`**

6.  Text within each **`<a>`** tag: Each link has a specific text (e.g., "A01AB04", "A07AA07", etc.), which is displayed as clickable text on the webpage.

7.  **`</p>`**: This is the closing tag for the paragraph.

In summary, here we have a list of hyperlinks pointing to different ATC codes, each with its own unique identifier. So in order to categorize each of our drugs, we can just extract the whatever is after "code=" within each link and we will get the information we want.

Knowing this, we can use `html_nodes()` and ask for it to find "strong:contains('ATC Code') + a" which looks for a **`<strong>`** tag containing "ATC Code" and then selects all **`<a>`** tags that are siblings (directly following) that **`<strong>`** tag. This will return our ATC codes.

```{r getting atc codes. test}
link2 <- "http://sideeffects.embl.de/drugs/1972/"

atc_codes<- link2 |>
  read_html() |>
  html_nodes("strong:contains('ATC Code') + a") |>
  html_text(trim = TRUE)
  
```

As we have been doing throughout this entire paper, we begin with a preliminary trial using just one link. We read the source code of the page using `read_html` and experimented with various XPath expressions until we found the one that captures the information we are interested in.

After figuring out the exact XPath, we store it in a function for its easy reuse and apply it to our matches data frame. This function extracts the ATC codes from a given URL. It reads the HTML content of the webpage, locates the element containing the ATC codes using XPath, and then extracts the text of the ATC codes. Finally, it returns the extracted ATC codes as a character vector.

```{r getting atc codes. function}
extrct_ATC_codes  <- function(url) {
  result <- url |> 
    read_html() |> 
    html_nodes("strong:contains('ATC Code') + a") |> 
    html_text(trim = TRUE)
  return(result)
}

```

This line of code applies the `extract_ATC_codes` function to each link in the `sampled_matches` dataset using `lapply`. It creates a new column named `atc_codes` in `sampled_matches`, where each entry corresponds to a list of ATC codes extracted from the respective link.

```{r SAMPLE getting atc codes. applying function, eval=FALSE}
sampled_matches$atc_codes <- lapply(sampled_matches$link,
                       extrct_ATC_codes)
```

```{r getting atc codes. applying function}
matches$atc_codes <- lapply(matches$link,
                       extrct_ATC_codes)
```

This code modifies the `atc_codes` column in the `sampled_matches` dataset, keeping only the first letter of each ATC code. This action is taken because we are primarily interested in the main categories represented by these letters, as they align with the broader classifications used in the OCDE data. By retaining only the first letter, we aim to mitigate potential biases that could arise from analyzing subcategories, which are not covered comprehensively in the OCDE dataset.

```{r SAMPLE getting atc codes. cleaning, eval=FALSE}
sampled_matches <- sampled_matches |> 
  mutate(atc_codes = substr(atc_codes, 1, 1))

sampled_matches
```

```{r getting atc codes. cleaning}
matches <- matches |> 
  mutate(atc_codes = substr(atc_codes, 1, 1))

head(matches)
```

## Using the data

Now that we have the data, we still have some steps before we can use it as intended.

### Wrangling

Notably, the data scrapped from the website does not come ready to be analyzed as its own. So here we work towards making it so we can use it.

The first thing we notice here is that the 'frequency' in which side effects show are not standardized. Some drugs record side effect occurrence as a range of a percentage, or others in more categorical ways using scales from less to more frequent. Naturally, this means we need to standardize this data.

For this, we will be referencing a conversion table used in this article in [Nature magazine](https://www.nature.com/articles/s41467-020-18305-y#Sec9) where they use similar data in the prediction of drug side effect occurrence. The values used for this mapping are as follows:

#### Mapping of frequency values to frequency classes and to frequency values:

| Frequency class | Intervention cohort- clinical trials frequency | Assigned value |
|-------------------|----------------------------------|-------------------|
| very frequent   | More than 10%                                  | 5              |
| frequent        | 1 to 10%                                       | 4              |
| infrequent      | 0.1 to 1%                                      | 3              |
| rare            | 0.01 to 0.1%                                   | 2              |
| very rare       | Less than 0.01%                                | 1              |
| zeros\*         | 0%                                             | 0              |

We initially process rows containing numerical values in the 'data_for_drug' column. By splitting this data into 'min' and 'max' columns and expressing them as decimals of a percentage, we calculate the occurrence frequency ('occu_freq'). If both 'min' and 'max' values are available, we calculate the mean; otherwise, we use whichever value is present. These frequency values are then scaled to a 0-100 scale, representing the range from 0 to 5 in frequency classes.

Subsequently, we assign an 'ass_value' based on predefined frequency class ranges. This ensures that very frequent occurrences receive the highest value (5), very rare or zero occurrences receive the lowest (0), and any remaining frequencies are labeled as NA.

```{r SAMPLE ass_values to numbers, eval=FALSE}
# Filter rows based on the presence of '-' or '%' in data_for_drug and absence of ','
sampled_filtered_rows <- side_effects_sample |> 
  filter(grepl("-|%", data_for_drug) & !grepl(",", data_for_drug)) |> 
  mutate(data_for_drug = gsub("%", "", data_for_drug)) |> 
  separate(data_for_drug, into = c("min", "max"), sep = "-") |> 
  mutate(across(c(min, max), ~as.numeric(.)*0.01)) |> 
  mutate(occu_freq = ifelse(is.na(max), min,
                            ifelse(is.na(min), max,
                                   (min + max) / 2)) * 100 / 5) |> 
  mutate(ass_value = case_when(
    occu_freq > 2 ~ 5,
    occu_freq >= 0.2 ~ 4,
    occu_freq >= 0.02 ~ 2,
    occu_freq >= 0.002 ~ 1,
    occu_freq == 0 ~ 0,
    TRUE ~ NA_real_
  )) |> 
  select(-c(min, max, occu_freq))

```

```{r ass_values to numbers}
# Filter rows based on the presence of '-' or '%' in data_for_drug and absence of ','
filtered_rows <- side_effects |> 
  filter(grepl("-|%", data_for_drug) & !grepl(",", data_for_drug)) |> 
  mutate(data_for_drug = gsub("%", "", data_for_drug)) |> 
  separate(data_for_drug, into = c("min", "max"), sep = "-") |> 
  mutate(across(c(min, max), ~as.numeric(.)*0.01)) |> 
  mutate(occu_freq = ifelse(is.na(max), min,
                            ifelse(is.na(min), max,
                                   (min + max) / 2)) * 100 / 5) |> 
  mutate(ass_value = case_when(
    occu_freq > 2 ~ 5,
    occu_freq >= 0.2 ~ 4,
    occu_freq >= 0.02 ~ 2,
    occu_freq >= 0.002 ~ 1,
    occu_freq == 0 ~ 0,
    TRUE ~ NA_real_
  )) |> 
  select(-c(min, max, occu_freq))

```

Next, we focus on extracting only words from the 'data_for_drug' column to further standardize the representation. We utilize a custom function, 'extract_words', to achieve this.

```{r ass_values to words. function}
# Function to extract ONLY words
extract_words <- function(text) {
  str_extract_all(text, "(?<!\\d)[a-zA-Z]+(?:\\s+[a-zA-Z]+)*") |> 
    map_chr(~ ifelse(length(.x) > 0, paste(.x, collapse = ", "), NA_character_))
}

```

Afterward, we filter out rows with NA values or those containing the word "to" in the 'words_only' column. Then, we split the words before and after the comma into two new columns and assign numerical values to these words based on a predefined table. Any remaining frequencies are labeled as NA, and the mean value between both columns is calculated. Finally, the 'ass_value' column stores the final value, which is the rounded mean to the highest value.

```{r SAMPLE ass_values to words, eval=FALSE}
# Apply the function to the data_for_drug column
sampled_filtered_rows2 <- side_effects_sample |> 
  mutate(words_only = extract_words(data_for_drug)) |> 
  filter(!is.na(words_only), 
         words_only != "to") |> 
# Splitting the words before and after the comma into two new columns
  separate(words_only, into = c("words_only", "after_comma"), 
           sep = ",", 
           remove = FALSE) |> 
  mutate(across(c(after_comma), trimws),
         across(c(words_only, after_comma), 
                ~ case_when(
                  . %in% c("very frequent", "very common") ~ 5,
                  . %in% c("frequent", "common") ~ 4,
                  . == "infrequent" ~ 3,
                  . %in% c("rare", "uncommon") ~ 2,
 . %in% c("very rare", "very uncommon") ~ 1,
                  TRUE ~ NA_real_)),
         mean_value = (words_only + after_comma) / 2,
         ass_value = ifelse(is.na(mean_value), words_only, ceiling(mean_value))) |> 
  select(-c(words_only, after_comma, mean_value, data_for_drug))

```

```{r ass_values to words}
# Apply the function to the data_for_drug column
filtered_rows2 <- side_effects |> 
  mutate(words_only = extract_words(data_for_drug)) |> 
  filter(!is.na(words_only), 
         words_only != "to") |> 
  # Splitting the words before and after the comma into two new columns
  separate(words_only, into = c("words_only", "after_comma"), 
           sep = ",", 
           remove = FALSE) |> 
  mutate(across(c(after_comma), trimws),
         across(c(words_only, after_comma), 
                ~ case_when(
                  . %in% c("very frequent", "very common") ~ 5,
                  . %in% c("frequent", "common") ~ 4,
                  . == "infrequent" ~ 3,
                  . %in% c("rare", "uncommon") ~ 2,
                  . %in% c("very rare", "very uncommon") ~ 1,
                  TRUE ~ NA_real_)),
         mean_value = (words_only + after_comma) / 2,
         ass_value = ifelse(is.na(mean_value), words_only, ceiling(mean_value))) |> 
  select(-c(words_only, after_comma, mean_value, data_for_drug))

```

This meticulous process ensures a consistent representation of side effects frequency and associated values for further analysis. To complete the data preparation, we combine these two datasets and merge them with the 'matches' dataset. In the final 'side_effects' table, we remove rows where 'ass_value' contains missing values and eliminate variables unnecessary for data analysis, such as 'html', 'link', and 'drug_id'.

```{r SAMPLE final side_effects, eval=FALSE}
# Bind and merge
sampled_side_effects <- bind_rows(sampled_filtered_rows, sampled_filtered_rows2)
sampled_side_effects <- merge(sampled_matches, sampled_side_effects, by = "drug_name", all = TRUE)

# Filter and clean side_effects
sampled_side_effects <- sampled_side_effects |> 
  na.omit(ass_values) |> 
  select(-c(hmtl, link, drug_id)) |> 
  arrange(atc_codes)
```

```{r final side_effects}
# Bind and merge
side_effects <- bind_rows(filtered_rows, filtered_rows2)
side_effects <- merge(matches, side_effects, by = "drug_name", all = TRUE)

# Filter and clean side_effects
side_effects <- side_effects |> 
  na.omit(ass_values) |> 
  select(-c(link, drug_id)) |> 
  arrange(atc_codes)

```

#### Matching ATC codes to drugs and indications

```{r SAMPLE match the ATC codes to indications, eval=FALSE}
temp_matches_sample <- sampled_side_effects |> 
  distinct(drug_name, atc_codes)

# Perform a left join to assign atc_code to each drug in indications_sample
indications_sample <- indications_sample |> 
  left_join(temp_matches_sample, by = "drug_name")

```

```{r match the ATC codes to indications}
temp_matches <- side_effects |> 
  distinct(drug_name, atc_codes)

# Perform a left join to assign atc_code to each drug in indications_sample
indications <- indications|> 
  left_join(temp_matches, by = "drug_name")

```

## Cleaning OECD data

[OECD Health Statistics database](http://www.oecd.org/health/health-data.htm) includes data on total pharmaceutical consumption according to the Anatomical Therapeutic Chemical (ATC) classification/Defined Daily Dose (DDD) system, created by the WHO Collaborating Centre for Drug Statistics Methodology. The ATC classification system divides drugs into different groups according to the organ system on which they act and/or therapeutical, pharmacological and chemical characteristics. The publication "ATC Index with DDDs" lists all assigned ATC codes and DDD values. Both publications are updated annually.

In the table below you can find a glimpse of the groups we have chosen for the current analysis. The ATC codes included are based on the 2023 version of the ATC Index.

|                                                                       |                        |
|------------------------------------------|------------------------------|
| **Main groups**                                                       | **Codes (2023 Index)** |
| A-Alimentary tract and metabolism                                     | A                      |
| B-Blood and blood forming organs                                      | B                      |
| C-Cardiovascular system                                               | C                      |
| G-Genito urinary system and sex hormones                              | G                      |
| H-Systemic hormonal preparations, excluding sex hormones and insulins | H                      |
| J-Antiinfectives for systemic use                                     | J                      |
| M-Musculo-skeletal system                                             | M                      |
| N-Nervous system Analgesics Anxiolytics                               | N                      |
| R-Respiratory system                                                  | R                      |

In their website they have a 'Data explorer**'** that allows you to pick variables, while also picking the country/years of data you want and download that custom data set for yourself. You can find the whole dataset [here](https://data-explorer.oecd.org/vis?tm=pharmaceutical%20consumption&pg=0&hc%5BMeasure%5D=Pharmaceutical%20consumption&snb=33&vw=tb&df%5Bds%5D=dsDisseminateFinalDMZ&df%5Bid%5D=HEALTH_PHMC%40DF_PHMC_CONSUM&df%5Bag%5D=OECD.ELS.HD&df%5Bvs%5D=1.0&pd=2021%2C&dq=.PH_CON...A&to%5BTIME_PERIOD%5D=false&ly%5Brw%5D=REF_AREA&ly%5Bcl%5D=TIME_PERIOD). However, we decided to download the whole data set and clean the data to keep only the information we are interested in:

-   'Year': we will be working with the data from the most recent year available,

-   'Country' and 'COU' (renamed as 'iso', since it is the ISO code),

-   'Variable': later separated into 'code', which we will be using for the analysis, and 'variable', a description of the code,

-   'Value': the unit of measure is Defined Daily Dosage (DDD) per 1.000 inhabitants per day. DDD is defined as the assumed average per maintenance dose per day for a drug used on its main indication in adults.

```{r}
ocde <- read.csv("HEALTH_PHMC.csv")

ocde <- ocde |> 
  select(-c(VAR, UNIT, Measure, YEA, Flag.Codes, Flags)) |>
  filter(Year == 2021) |> 
  rename(ISO = COU) |> 
  separate(Variable, 
           into = c("atc_codes", "Variable"), 
           sep = "-", 
           extra = "merge") |> 
  filter(nchar(atc_codes) == 1) |> 
  clean_names() 
```

## Accessing side effects

Lets look at drug categories: the following shiny app allows us to interact with the data we scraped and see the side effects and indications sorted by ATC code or specific pharmaceutical drug as well as the ranking of OECD countries by their intake.

```{r we make a cool dashboard}
# Install required packages if not already installed
if (!requireNamespace("shiny", quietly = TRUE)) {
  install.packages("shiny")
}
if (!requireNamespace("plotly", quietly = TRUE)) {
  install.packages("plotly")
}
if (!requireNamespace("dplyr", quietly = TRUE)) {
  install.packages("dplyr")
}
if (!requireNamespace("htmlwidgets", quietly = TRUE)) {
  install.packages("htmlwidgets")
}

# Load required libraries
library(shiny)
library(plotly)
library(dplyr)
library(htmlwidgets)

# Define UI
ui <- fluidPage(
  titlePanel("Interactive Dashboard with ATC Codes"),
  sidebarLayout(
    sidebarPanel(
      selectInput("atc_codes", "Select ATC Code:", choices = unique(side_effects$atc_codes)),
      selectInput("selected_drug", "Select Drug:", choices = c(NULL, unique(side_effects$drug_name))),
      width = 3
    ),
    mainPanel(
      tabsetPanel(
        tabPanel("Drug side effects and indications", 
                 tableOutput("top_drug_names"),
                 tableOutput("indications_table")),  
        # New table for indications
        tabPanel("Top 10 Side Effects", plotlyOutput("top_side_effects_chart")),
        tabPanel("Country ranking by population intake", plotlyOutput("country_ranking_chart"))
      ),
      width = 9
    )
  )
)

# Define server
server <- function(input, output, session) {
  
  # Reactive function for available drugs based on selected ATC code
  observe({
    atc_filtered_drugs <- if (!is.null(input$atc_codes)) {
      side_effects |>
        filter(atc_codes == input$atc_codes) |>
        select(drug_name) |>
        unique()
    } else {
      NULL
    }
    
    drug_choices <- c("No selection", atc_filtered_drugs$drug_name)
    
    updateSelectInput(session, "selected_drug", choices = drug_choices)
  })
  
  # Filter side effects data based on user input
  filtered_side_effects_data <- reactive({
    if (!is.null(input$selected_drug) && input$selected_drug != "No selection") {
      side_effects |>
        filter(atc_codes == input$atc_codes, drug_name == input$selected_drug)
    } else {
      side_effects |>
        filter(atc_codes == input$atc_codes)
    }
  })
  
  # Filter indications data based on user input
  filtered_indications_data <- reactive({
    if (!is.null(input$selected_drug) && input$selected_drug != "No selection") {
      indications |>
        filter(atc_codes == input$atc_codes, drug_name == input$selected_drug)
    } else {
      indications |>
        filter(atc_codes == input$atc_codes)
    }
  })
  
  # Top 10 drug names with most side effects
  output$top_drug_names <- renderTable({
    top_drug_names <- filtered_side_effects_data() |>
      group_by(drug_name) |>
      summarize(side_effect_count = n()) |>
      arrange(desc(side_effect_count)) |>
      head(10)
    top_drug_names
  })
  
  # Table of indications ordered from most to least common
  output$indications_table <- renderTable({
    indications_table <- filtered_indications_data() |>
      group_by(indication) |>
      summarize(indications_count = sum(1)) |>
      arrange(desc(indications_count))
    indications_table
  })
  
  # Top 10 side effects bar chart with count * mean(ass_value)
  output$top_side_effects_chart <- renderPlotly({
    top_side_effects <- filtered_side_effects_data() |>
      group_by(side_effect) |>
      summarize(count_times_mean = n() * mean(ass_value)) |>
      arrange(desc(count_times_mean)) |>
      head(10)
    
    # Sort the data frame from largest to smallest
    top_side_effects <- top_side_effects[order(top_side_effects$count_times_mean, decreasing = TRUE), ]
    
    plot_ly(top_side_effects, x = ~side_effect, y = ~count_times_mean, type = 'bar') |>
      layout(title = "Top Side Effects)",
             xaxis = list(title = "Side Effect"),
             yaxis = list(title = "Count * Mean ass_value"))
  })
  
  # Filter OCDE data based on user input
  filtered_ocde_data <- reactive({
    ocde |>
      filter(atc_codes == input$atc_codes)
  })
   
  # Country ranking bar chart
  output$country_ranking_chart <- renderPlotly({
    country_ranking <- filtered_ocde_data() |>
      group_by(country) |>
      summarize(max_value = max(value)) |>
      arrange(desc(max_value))
    
    plot_ly(country_ranking, x = ~country, y = ~max_value, type = 'bar') |>
      layout(title = "Country Ranking",
             xaxis = list(title = "Country"),
             yaxis = list(title = "Daily Dosage per 1k inhabitants per day"))
  })
}

# Run the application
shinyApp(ui = ui, server = server)

```

## Visualization

For this visualization we created an interactive map. Our process began with data cleaning, which involved merging the OECD dataset with the indications and side_effects tables. Due to the complexity of the data for direct mapping, we made several decisions to streamline the information:

-   We retained only the ATC code with the highest value for each country. This allowed us to focus on the most significant medication usage within each country.

```{r data for map. 1st}
# Prepare ocde data
ocde_max <- ocde |> 
  group_by(iso) |> 
  mutate(max_value = max(value))|> 
  filter(value == max_value) |> 
  select(country, iso, atc_codes, variable, max_value) 

```

-   Within each ATC code, we identified the drug with the highest number of indications. This helped prioritize drugs that are widely prescribed or used for diverse medical conditions.
-   For each selected drug, we filtered out side effects with a frequency equal to or higher than 4. This ensured we focused on the most common and potentially significant side effects.

```{r data for map}
# Get the drug with the most indications
max_indications_drug <- indications |> 
  group_by(drug_name) |> 
  summarize(total_indications = n()) |> 
  inner_join(side_effects, by = "drug_name") |> 
  group_by(atc_codes) |> 
  filter(total_indications == max(total_indications),
         ass_value >= 4)

```

-   From the filtered side effects, we randomly chose 5 to include in the visualization. This provided a representative sample of side effects associated with each drug. Additionally, we ensured that if there were multiple drugs within each ATC code, only one of them was randomly chosen.

```{r data for map}
# Join with oecd data 
map <- ocde_max |> 
  inner_join(max_indications_drug, by = "atc_codes") |> 
  group_by(country) |> 
  mutate(drug_name = if (n_distinct(drug_name) > 1) sample(drug_name, 1) else drug_name) |>
  ungroup() |> 
  group_by(country, atc_codes, drug_name) |> 
  slice_sample(n = 5, replace = FALSE)

```

Now we can make the map.

```{r}
# Install necessary packages if needed
if (!requireNamespace("leaflet", quietly = TRUE)) {
  install.packages("leaflet")
}
if (!requireNamespace("rnaturalearth", quietly = TRUE)) {
  install.packages("rnaturalearth")
}
if (!requireNamespace("rnaturalearthdata", quietly = TRUE)) {
  install.packages("rnaturalearthdata")
}


# Load required libraries
library(leaflet)
library(rnaturalearth)
library(rnaturalearthdata)

# Load world map data
world_map <- ne_countries(returnclass = "sf")

# Merge with our data
world_map <- left_join(world_map, map, 
                       by = c("name" = "country"))

# Create popup content for each country. It ensures that all the elements in each variable are included
popup_content <- lapply(1:nrow(world_map), function(i) {
  country <- world_map$name[i]
  drug_name <- world_map$drug_name[i]
  side_effects <- world_map$side_effect[world_map$name == country]
  variable <- world_map$variable[i]
  
  content <- paste("<strong>Country:</strong>", country, "<br>",
                   "<strong>Drug Name:</strong>", drug_name, "<br>",
                   "<strong>Side Effects:</strong>", paste(side_effects, collapse = ", "), "<br>",
                   "<strong>Use:</strong>", variable, "<br>")
  return(content)
})


# Gradient color palette based on value
colors <- colorNumeric(palette = "viridis", domain = world_map$max_value)


# Map
leaflet(data = world_map) |> 
  setView(lng = 20, lat = 30, zoom = 1.4) |> 
  addProviderTiles("OpenStreetMap.Mapnik") |> 
  addPolygons(fillColor = ~colors(max_value),
              fillOpacity = 0.7,
              weight = 1,
              popup = popup_content,
              highlightOptions = highlightOptions(color = "black", weight = 2, bringToFront = TRUE))


```
